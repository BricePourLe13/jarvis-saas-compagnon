# Code Examples - Agent Vocal Realtime (Pr√™t √† utiliser)

## üìö Contenu

1. [Serveur Backend - Token Ephemeral](#serveur-backend---token-√©ph√©m√®re)
2. [Frontend Web - Miroir](#frontend-web---miroir-num√©rique)
3. [Python avec Agents SDK](#python-avec-agents-sdk)
4. [Tests & Debugging](#tests--debugging)

---

## Serveur Backend - Token √âph√©m√®re

### Express.js + Node.js

```javascript
// backend/server.js
const express = require("express");
const cors = require("cors");
const OpenAI = require("openai");

const app = express();
app.use(cors());
app.use(express.json());

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// üìå Route pour g√©n√©rer token √©ph√©m√®re (1 min expiration)
app.post("/api/session", async (req, res) => {
  try {
    const session = await openai.realtime.sessions.create({
      model: "gpt-realtime",
      voice: "cedar",
    });

    // ‚úÖ R√©pondre avec token court-dur√©e
    res.json({
      client_secret: session.client_secret.value,
      expires_at: session.client_secret.expires_at,
      session_id: session.id,
    });
  } catch (error) {
    console.error("‚ùå Erreur cr√©ation session:", error);
    res.status(500).json({ error: "Session creation failed" });
  }
});

// üìå Route pour logging (s√©curit√©)
app.post("/api/log", (req, res) => {
  const { event, session_id, error } = req.body;
  console.log({
    timestamp: new Date().toISOString(),
    event,
    session_id,
    error,
  });
  res.json({ logged: true });
});

// üìå Health check
app.get("/health", (req, res) => {
  res.json({ status: "ok", timestamp: Date.now() });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`‚úÖ Backend Realtime API server running on port ${PORT}`);
});
```

**Lancer**:
```bash
export OPENAI_API_KEY=sk-...
npm install express cors openai
node backend/server.js
```

---

## Frontend Web - Miroir Num√©rique

### HTML + TypeScript (Electron ou Browser)

```html
<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Agent Vocal - Salle de Sport</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Arial, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    .mirror-container {
      width: 100%;
      max-width: 800px;
      background: white;
      border-radius: 20px;
      box-shadow: 0 20px 60px rgba(0,0,0,0.3);
      padding: 40px;
      text-align: center;
    }
    
    .status {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 10px;
      margin-bottom: 20px;
      font-size: 14px;
      color: #666;
    }
    
    .status-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #ccc;
      transition: all 0.3s;
    }
    
    .status-dot.connected {
      background: #4ade80;
      box-shadow: 0 0 10px #4ade80;
    }
    
    .status-dot.listening {
      background: #ef4444;
      animation: pulse 1s infinite;
    }
    
    .status-dot.responding {
      background: #3b82f6;
      animation: pulse 1s infinite;
    }
    
    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }
    
    .voice-indicator {
      width: 200px;
      height: 200px;
      margin: 30px auto;
      border-radius: 50%;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 60px;
    }
    
    .voice-indicator.active {
      animation: wave 0.8s infinite;
    }
    
    @keyframes wave {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.1); }
    }
    
    .transcript {
      min-height: 60px;
      font-size: 18px;
      color: #333;
      line-height: 1.6;
      margin: 20px 0;
      padding: 15px;
      background: #f5f5f5;
      border-radius: 10px;
    }
    
    .transcript.empty {
      color: #aaa;
      font-style: italic;
    }
    
    .button-group {
      display: flex;
      gap: 10px;
      justify-content: center;
      margin-top: 30px;
    }
    
    button {
      padding: 12px 30px;
      border: none;
      border-radius: 8px;
      font-size: 16px;
      cursor: pointer;
      transition: all 0.3s;
    }
    
    .btn-primary {
      background: #667eea;
      color: white;
    }
    
    .btn-primary:hover {
      background: #5568d3;
      transform: scale(1.05);
    }
    
    .btn-secondary {
      background: #e5e7eb;
      color: #333;
    }
    
    .btn-secondary:hover {
      background: #d1d5db;
    }
  </style>
</head>
<body>
  <div class="mirror-container">
    <h1>üèãÔ∏è Agent Fitness Vocal</h1>
    
    <div class="status">
      <div class="status-dot" id="statusDot"></div>
      <span id="statusText">Connexion...</span>
    </div>
    
    <div class="voice-indicator" id="voiceIndicator">
      üé§
    </div>
    
    <div class="transcript empty" id="transcript">
      Posez votre question...
    </div>
    
    <div class="button-group">
      <button class="btn-primary" id="resetBtn">Nouvelle question</button>
      <button class="btn-secondary" id="debugBtn">Debug (console)</button>
    </div>
  </div>

  <script type="module">
    // ============================================
    // REALTIME VOICE AGENT - FRONTEND
    // ============================================
    
    // Configuration
    const CONFIG = {
      API_BASE: "http://localhost:3000",
      MODEL: "gpt-realtime",
      VOICE: "cedar",
      SAMPLE_RATE: 24000,
      VAD_THRESHOLD: 0.4, // Bas pour bruit ambiant
      CHUNK_SIZE: 4096,
    };
    
    // √âtat global
    const state = {
      ws: null,
      audioContext: null,
      audioProcessor: null,
      audioOutput: null,
      sessionId: null,
      isConnected: false,
      isListening: false,
      isResponding: false,
      sessionToken: null,
    };
    
    // ========== UTILS ==========
    const UI = {
      statusDot: document.getElementById("statusDot"),
      statusText: document.getElementById("statusText"),
      voiceIndicator: document.getElementById("voiceIndicator"),
      transcript: document.getElementById("transcript"),
      resetBtn: document.getElementById("resetBtn"),
      debugBtn: document.getElementById("debugBtn"),
      
      updateStatus(status, text) {
        this.statusDot.className = `status-dot ${status}`;
        this.statusText.textContent = text;
      },
      
      setTranscript(text, isUser = false) {
        this.transcript.textContent = text;
        this.transcript.classList.toggle("empty", !text);
      },
      
      setVoiceActive(active) {
        this.voiceIndicator.classList.toggle("active", active);
      },
    };
    
    const Logger = {
      log(...args) {
        console.log("[Realtime Agent]", ...args);
      },
      error(...args) {
        console.error("[Realtime Agent]", ...args);
      },
      warn(...args) {
        console.warn("[Realtime Agent]", ...args);
      },
      
      async logServer(event, error = null) {
        try {
          await fetch(`${CONFIG.API_BASE}/api/log`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              event,
              session_id: state.sessionId,
              error,
              timestamp: new Date().toISOString(),
            }),
          });
        } catch (e) {
          console.error("Logging failed:", e);
        }
      },
    };
    
    // ========== AUDIO ==========
    const Audio = {
      float32ToPCM16(float32Array) {
        const pcm16 = new Int16Array(float32Array.length);
        for (let i = 0; i < float32Array.length; i++) {
          const s = Math.max(-1, Math.min(1, float32Array[i]));
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
        }
        return pcm16;
      },
      
      arrayBufferToBase64(buffer) {
        let binary = "";
        const bytes = new Uint8Array(buffer);
        for (let i = 0; i < bytes.byteLength; i++) {
          binary += String.fromCharCode(bytes[i]);
        }
        return btoa(binary);
      },
      
      base64ToArrayBuffer(base64) {
        const binary = atob(base64);
        const bytes = new Uint8Array(binary.length);
        for (let i = 0; i < binary.length; i++) {
          bytes[i] = binary.charCodeAt(i);
        }
        return bytes.buffer;
      },
      
      async initContext() {
        if (state.audioContext) return;
        
        state.audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: CONFIG.SAMPLE_RATE,
        });
        await state.audioContext.resume();
        Logger.log("‚úÖ Audio context initialized");
      },
      
      async initMicrophone() {
        await this.initContext();
        
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: false, // ‚ö†Ô∏è Important pour VAD
          },
        });
        
        const source = state.audioContext.createMediaStreamSource(stream);
        const processor = state.audioContext.createScriptProcessor(CONFIG.CHUNK_SIZE, 1, 1);
        
        processor.onaudioprocess = (event) => {
          if (!state.isConnected || state.ws.readyState !== WebSocket.OPEN) return;
          
          const float32 = event.inputBuffer.getChannelData(0);
          const pcm16 = this.float32ToPCM16(float32);
          const base64 = this.arrayBufferToBase64(pcm16.buffer);
          
          state.ws.send(JSON.stringify({
            type: "input_audio_buffer.append",
            audio: base64,
          }));
        };
        
        source.connect(processor);
        processor.connect(state.audioContext.destination);
        
        Logger.log("‚úÖ Microphone initialized");
      },
      
      initAudioOutput() {
        state.audioOutput = {
          buffers: [],
          isPlaying: false,
          source: null,
        };
        Logger.log("‚úÖ Audio output initialized");
      },
      
      playAudio(base64Audio) {
        const buffer = this.base64ToArrayBuffer(base64Audio);
        const audioBuffer = state.audioContext.createBuffer(
          1,
          buffer.byteLength / 2,
          CONFIG.SAMPLE_RATE
        );
        
        const data = audioBuffer.getChannelData(0);
        const view = new Int16Array(buffer);
        for (let i = 0; i < view.length; i++) {
          data[i] = view[i] / 32768.0;
        }
        
        const source = state.audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(state.audioContext.destination);
        source.start(0);
      },
    };
    
    // ========== WEBSOCKET ==========
    const Realtime = {
      async connect() {
        try {
          // 1. R√©cup√©rer token √©ph√©m√®re
          const response = await fetch(`${CONFIG.API_BASE}/api/session`, {
            method: "POST",
          });
          const data = await response.json();
          state.sessionToken = data.client_secret;
          state.sessionId = data.session_id;
          
          Logger.log("‚úÖ Session token obtenu:", state.sessionId);
          
          // 2. √âtablir WebSocket
          const url = `wss://api.openai.com/v1/realtime?model=${CONFIG.MODEL}`;
          state.ws = new WebSocket(url, [
            "realtime",
            `openai-insecure-api-key.${state.sessionToken}`,
          ]);
          
          state.ws.onopen = () => this.onOpen();
          state.ws.onmessage = (event) => this.onMessage(JSON.parse(event.data));
          state.ws.onerror = (error) => this.onError(error);
          state.ws.onclose = () => this.onClose();
          
        } catch (error) {
          Logger.error("‚ùå Connection failed:", error);
          Logger.logServer("connection_failed", error.message);
          UI.updateStatus("disconnected", "Erreur connexion");
        }
      },
      
      onOpen() {
        Logger.log("‚úÖ WebSocket connect√©");
        state.isConnected = true;
      },
      
      onMessage(event) {
        const { type } = event;
        
        // Logger tous les √©v√©nements (debug)
        Logger.log(`üì® Event: ${type}`);
        
        switch (type) {
          case "session.created":
            this.handleSessionCreated(event);
            break;
          
          case "session.updated":
            this.handleSessionUpdated(event);
            break;
          
          case "input_audio_buffer.speech_started":
            this.handleSpeechStarted();
            break;
          
          case "input_audio_buffer.speech_stopped":
            this.handleSpeechStopped();
            break;
          
          case "response.output_audio.delta":
            this.handleAudioDelta(event);
            break;
          
          case "response.output_audio_transcript.delta":
            this.handleTranscriptDelta(event);
            break;
          
          case "response.done":
            this.handleResponseDone(event);
            break;
          
          case "error":
            this.handleError(event);
            break;
        }
      },
      
      handleSessionCreated(event) {
        Logger.log("‚úÖ Session created", event.session.id);
        UI.updateStatus("connected", "Pr√™t");
        
        // Configuration session
        this.updateSession();
      },
      
      updateSession() {
        state.ws.send(JSON.stringify({
          type: "session.update",
          session: {
            type: "realtime",
            instructions: `Tu es un assistant vocal pour une salle de sport.
Tu aides les adh√©rents avec:
- Horaires des cours
- Informations √©quipements
- Conseils fitness
- R√©servations

R√©ponds en fran√ßais, sois amical et concis (< 30s).`,
            voice: CONFIG.VOICE,
            output_modalities: ["audio"],
            audio: {
              input: {
                format: { type: "audio/pcm", rate: CONFIG.SAMPLE_RATE },
                transcription: { model: "whisper-1" },
                turn_detection: {
                  type: "server_vad",
                  threshold: CONFIG.VAD_THRESHOLD,
                  silence_duration_ms: 500,
                  prefix_padding_ms: 300,
                  create_response: true,
                },
              },
              output: {
                voice: CONFIG.VOICE,
                format: { type: "audio/pcm", rate: CONFIG.SAMPLE_RATE },
              },
            },
          },
        }));
      },
      
      handleSessionUpdated(event) {
        Logger.log("‚úÖ Session configur√©e");
        UI.updateStatus("connected", "Pr√™t - Parlez!");
        Logger.logServer("session_configured");
      },
      
      handleSpeechStarted() {
        state.isListening = true;
        UI.updateStatus("listening", "√âcoute...");
        UI.setVoiceActive(true);
        UI.setTranscript("En attente...");
      },
      
      handleSpeechStopped() {
        state.isListening = false;
        UI.updateStatus("responding", "Traitement...");
        UI.setTranscript("Traitement de votre demande...");
      },
      
      handleAudioDelta(event) {
        state.isResponding = true;
        UI.setVoiceActive(true);
        Audio.playAudio(event.delta);
      },
      
      handleTranscriptDelta(event) {
        UI.setTranscript(event.transcript || "");
      },
      
      handleResponseDone(event) {
        state.isResponding = false;
        state.isListening = false;
        UI.updateStatus("connected", "Pr√™t - Parlez!");
        UI.setVoiceActive(false);
        Logger.logServer("response_completed");
      },
      
      handleError(event) {
        Logger.error("‚ùå API Error:", event.error);
        UI.updateStatus("error", `Erreur: ${event.error.message}`);
        Logger.logServer("api_error", event.error.message);
      },
      
      onError(error) {
        Logger.error("‚ùå WebSocket error:", error);
        Logger.logServer("websocket_error", error.message);
      },
      
      onClose() {
        state.isConnected = false;
        UI.updateStatus("disconnected", "D√©connect√©");
        Logger.warn("‚ö†Ô∏è WebSocket ferm√© - Reconnexion en 5s...");
        Logger.logServer("connection_closed");
        
        setTimeout(() => this.connect(), 5000);
      },
      
      reset() {
        if (state.ws && state.ws.readyState === WebSocket.OPEN) {
          state.ws.close();
        }
        state.ws = null;
        UI.setTranscript("Nouvelle question...");
        this.connect();
      },
    };
    
    // ========== INIT ==========
    (async function init() {
      try {
        // Initialiser audio
        await Audio.initContext();
        await Audio.initMicrophone();
        Audio.initAudioOutput();
        
        // Connecter Realtime API
        await Realtime.connect();
        
        // Event listeners UI
        UI.resetBtn.addEventListener("click", () => Realtime.reset());
        UI.debugBtn.addEventListener("click", () => {
          Logger.log("=== DEBUG INFO ===");
          Logger.log("State:", state);
          Logger.log("Config:", CONFIG);
        });
        
      } catch (error) {
        Logger.error("‚ùå Initialization failed:", error);
        UI.updateStatus("error", "Erreur d'initialisation");
      }
    })();
  </script>
</body>
</html>
```

**Utilisation**:
1. Sauvegarder comme `frontend/index.html`
2. Lancer backend: `node backend/server.js`
3. Ouvrir dans navigateur: `http://localhost:8080/frontend/index.html`

---

## Python avec Agents SDK

### Installation
```bash
pip install openai-agents openai sounddevice numpy
```

### Code complet
```python
# voice_agent.py
import asyncio
import numpy as np
import sounddevice as sd
import io
from agents.realtime import RealtimeAgent, RealtimeRunner
from agents import Agent
from agents import tool

# ============================================
# AGENT D√âFINITION
# ============================================

agent = RealtimeAgent(
    name="Assistant Fitness",
    instructions="""Tu es un assistant vocal pour une salle de sport.
Tu aides les adh√©rents avec leurs questions sur:
- Les horaires des cours
- Les √©quipements disponibles
- Les conseils fitness
- Les r√©servations de cours

R√©ponds toujours en fran√ßais.
Sois amical et enthousiaste.
Garde les r√©ponses courtes et concises (moins de 30 secondes de parole).

Si l'utilisateur demande des informations sp√©cifiques (horaires, prix),
tu peux faire appel √† des outils pour consulter la base de donn√©es."""
)

# ============================================
# TOOLS (Int√©grations possibles)
# ============================================

@tool
def get_class_schedule(class_name: str) -> str:
    """Obtient l'horaire d'un cours sp√©cifique"""
    schedules = {
        "yoga": "Lundi et jeudi √† 18h",
        "spinning": "Lundi/Mercredi/Vendredi √† 17h et 19h",
        "musculation": "Ouvert 24h/24",
        "aquagym": "Mardi et jeudi √† 19h",
    }
    return schedules.get(class_name.lower(), "Cours non trouv√©")

@tool
def get_gym_hours() -> str:
    """Obtient les horaires d'ouverture"""
    return "Lundi-Vendredi: 6h-22h, Samedi: 8h-20h, Dimanche: 9h-18h"

@tool
def reserve_class(class_name: str, user_email: str) -> str:
    """R√©serve une place dans un cours"""
    # Simulation - remplacer par vrai syst√®me
    return f"‚úÖ R√©servation confirm√©e pour {class_name}"

# Ajouter les outils √† l'agent
agent.add_tool(get_class_schedule)
agent.add_tool(get_gym_hours)
agent.add_tool(reserve_class)

# ============================================
# SESSION CONFIGURATION
# ============================================

config = {
    "model_settings": {
        "model_name": "gpt-realtime",
        "voice": "cedar",
        "modalities": ["audio"],
        "input_audio_format": "pcm16",
        "output_audio_format": "pcm16",
        "input_audio_transcription": {"model": "whisper-1"},
        "turn_detection": {
            "type": "server_vad",
            "threshold": 0.4,  # Bas pour bruit ambiant (salle de sport)
            "silence_duration_ms": 500,
            "prefix_padding_ms": 300,
            "interrupt_response": True,
        },
    }
}

# ============================================
# RUNNER & SESSION
# ============================================

async def main():
    runner = RealtimeRunner(
        starting_agent=agent,
        config=config,
    )
    
    session = await runner.run()
    
    async with session:
        print("üé§ Assistant Fitness vocal actif")
        print("üí¨ Parlez librement - Appuyez sur Ctrl+C pour arr√™ter\n")
        
        try:
            async for event in session:
                # Traiter les √©v√©nements
                if event.type == "agent_start":
                    print(f"‚úÖ {event.agent.name} d√©marr√©")
                
                elif event.type == "audio_end":
                    print("‚úÖ R√©ponse vocale termin√©e\n")
                
                elif event.type == "tool_start":
                    print(f"üîß Outil: {event.tool.name}")
                
                elif event.type == "tool_end":
                    print(f"‚úÖ {event.tool.name}: {event.output}\n")
                
                elif event.type == "error":
                    print(f"‚ùå Erreur: {event.error}")
                
                # Ignorer les √©v√©nements fr√©quents
                elif event.type in ["history_updated", "history_added", "raw_model_event"]:
                    pass
                
                else:
                    # Log des autres √©v√©nements (debug)
                    if event.type.startswith("audio"):
                        pass  # Trop verbose
                    else:
                        print(f"üì® {event.type}")
        
        except KeyboardInterrupt:
            print("\nüëã Session termin√©e")

# ============================================
# LANCER
# ============================================

if __name__ == "__main__":
    print("üöÄ D√©marrage du serveur vocal Realtime...\n")
    asyncio.run(main())
```

**Lancer**:
```bash
export OPENAI_API_KEY=sk-...
python voice_agent.py
```

---

## Tests & Debugging

### Test WebSocket brut (Node.js)
```javascript
// test-ws.js
const WebSocket = require("ws");

async function test() {
  const sessionToken = "sk_live_..."; // √Ä obtenir de /api/session
  
  const ws = new WebSocket(
    "wss://api.openai.com/v1/realtime?model=gpt-realtime",
    ["realtime", `openai-insecure-api-key.${sessionToken}`]
  );
  
  ws.on("open", () => {
    console.log("‚úÖ Connect√©");
    
    // Attendre session.created
  });
  
  ws.on("message", (data) => {
    const event = JSON.parse(data);
    console.log("üì®", event.type);
  });
  
  ws.on("error", (error) => {
    console.error("‚ùå", error);
  });
}

test();
```

### V√©rifier configuration
```bash
# Test latence
curl -w "Response time: %{time_total}s\n" \
  https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# Test cl√© API
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY" | jq '.data[] | select(.id | contains("realtime"))'
```

---

## Checklist avant production

- [ ] Variables d'env s√©curis√©es (pas en hardcoded)
- [ ] Logging structur√© (timestamps, session IDs)
- [ ] Gestion d'erreurs WebSocket + reconnect
- [ ] Tests avec bruit ambiant (musique, gens)
- [ ] Tests latence r√©seau faible
- [ ] Monitoring uptime/erreurs
- [ ] Rate limiting si plusieurs miroirs
- [ ] Authentification adh√©rents (ID badge)
- [ ] Cache r√©ponses fr√©quentes
- [ ] Tests de charge (10+ sessions simultan√©es)

---

Vous avez maintenant un syst√®me **compl√®tement fonctionnel** √† d√©ployer ! üöÄ
