"use client"

import { useState, useCallback, useRef, useEffect } from 'react'
import { executeJarvisFunction } from '@/lib/jarvis-expert-functions'

interface VoiceVitrineConfig {
  onStatusChange?: (status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => void
  onTranscriptUpdate?: (transcript: string) => void
  maxDuration?: number // en secondes
}

export function useVoiceVitrineChat({
  onStatusChange,
  onTranscriptUpdate,
  maxDuration = 120
}: VoiceVitrineConfig) {
  // √âtats
  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  
  // Refs pour WebRTC
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const sessionStartTimeRef = useRef<number | null>(null)
  const maxDurationRef = useRef(maxDuration)

  // Mettre √† jour maxDuration
  useEffect(() => {
    maxDurationRef.current = maxDuration
  }, [maxDuration])

  const updateStatus = useCallback((status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => {
    onStatusChange?.(status)
  }, [onStatusChange])

  const updateTranscript = useCallback((transcript: string) => {
    setCurrentTranscript(transcript)
    onTranscriptUpdate?.(transcript)
  }, [onTranscriptUpdate])

  // üéØ Handler pour les function calls (ROI, success stories, etc.)
  const handleFunctionCall = useCallback(async (message: any, dataChannel: RTCDataChannel) => {
    try {
      const { call_id, name, arguments: argsString } = message
      console.log(`üîß Ex√©cution function: ${name}`)
      console.log(`üìä Arguments:`, argsString)
      
      // Parser les arguments
      const args = JSON.parse(argsString)
      
      // Ex√©cuter la fonction experte
      const result = await executeJarvisFunction(name, args)
      console.log(`‚úÖ R√©sultat function ${name}:`, result)
      
      // Renvoyer le r√©sultat √† l'IA
      dataChannel.send(JSON.stringify({
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: call_id,
          output: JSON.stringify(result)
        }
      }))
      
      // Demander √† l'IA de r√©pondre avec ce r√©sultat
      dataChannel.send(JSON.stringify({
        type: 'response.create'
      }))
      
      console.log('üì§ R√©sultat envoy√© √† JARVIS pour formulation')
      
    } catch (error) {
      console.error('‚ùå Erreur ex√©cution function call:', error)
      
      // En cas d'erreur, informer l'IA
      if (message.call_id) {
        dataChannel.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'function_call_output',
            call_id: message.call_id,
            output: JSON.stringify({ 
              error: 'Erreur lors du calcul, je vais vous donner une estimation g√©n√©rale.' 
            })
          }
        }))
        
        dataChannel.send(JSON.stringify({
          type: 'response.create'
        }))
      }
    }
  }, [])

  // Cr√©er une session √©ph√©m√®re pour la d√©mo
  const createDemoSession = useCallback(async () => {
    // L'API attend maintenant directement la config sans wrapper "session"
    const response = await fetch('/api/voice/vitrine/session', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({}), // L'API cr√©e la config elle-m√™me maintenant
    })

    if (!response.ok) {
      // üîí NOUVEAU : G√©rer les erreurs de limitation
      const errorData = await response.json().catch(() => ({}))
      
      const error: any = new Error(errorData.error || `Erreur session: ${response.status}`)
      error.statusCode = response.status
      error.hasActiveSession = errorData.hasActiveSession
      error.remainingCredits = errorData.remainingCredits
      error.isBlocked = errorData.isBlocked
      
      throw error
    }

    const sessionData = await response.json()
    console.log('‚úÖ Session cr√©√©e:', sessionData)
    console.log('üîç Structure session:', JSON.stringify(sessionData.session, null, 2))
    console.log('üîç client_secret:', sessionData.session?.client_secret)
    
    // üí≥ Retourner aussi les cr√©dits restants
    if (sessionData.remainingCredits !== undefined) {
      console.log(`üí≥ Cr√©dits restants: ${sessionData.remainingCredits} minutes`)
    }
    
    return sessionData
  }, [])

  // Initialiser WebRTC
  const initializeWebRTC = useCallback(async () => {
    try {
      // V√©rifier support WebRTC
      if (!window.RTCPeerConnection) {
        throw new Error('WebRTC non support√© par ce navigateur')
      }

      // Cr√©er session d√©mo
      const sessionResponse = await createDemoSession()
      const session = sessionResponse.session
      console.log('üîç Session utilis√©e:', session)
      
      // Configurer peer connection
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
      })
      peerConnectionRef.current = pc

      // Cr√©er l'√©l√©ment audio pour le playback (COMME LE KIOSK)
      if (!audioElementRef.current) {
        const audioEl = document.createElement('audio')
        audioEl.autoplay = true
        audioElementRef.current = audioEl
      }

      // G√©rer l'audio entrant (r√©ponses de JARVIS) - COMME LE KIOSK
      pc.ontrack = (event) => {
        console.log('üîä Audio entrant re√ßu (style kiosk)')
        if (audioElementRef.current && event.streams[0]) {
          audioElementRef.current.srcObject = event.streams[0]
          console.log('‚úÖ Audio srcObject d√©fini')
        }
      }

      // Demander acc√®s microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 24000, // Qualit√© HD pour meilleur rendu
            channelCount: 1,
            latency: 0.01, // Faible latence
            volume: 1.0
          }
        })
        
        // Ajouter track audio
        const audioTrack = stream.getAudioTracks()[0]
        pc.addTrack(audioTrack, stream)
      } catch (micError) {
        throw new Error('MICROPHONE_PERMISSION_DENIED')
      }

      // Configurer data channel
      const dc = pc.createDataChannel('oai-events')
      dataChannelRef.current = dc

      dc.onopen = () => {
        console.log('‚úÖ Data channel ouvert')
        setIsConnected(true)
        updateStatus('connected')
        sessionStartTimeRef.current = Date.now()
        
        // ‚úÖ CORRECTION : Ne pas √©craser le prompt serveur !
        // Le prompt commercial expert est d√©j√† configur√© dans /api/voice/vitrine/session
        // avec toutes les instructions d√©taill√©es, function calling, etc.
        console.log('üéØ Utilisation du prompt commercial serveur (expert JARVIS-GROUP)')
      }

      dc.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          console.log('üì® Message re√ßu:', message.type)
          
          switch (message.type) {
            case 'input_audio_buffer.speech_started':
              updateStatus('listening')
              break
              
            case 'input_audio_buffer.speech_stopped':
              updateStatus('connected')
              break
              
            case 'response.created':
              updateStatus('speaking')
              setIsAISpeaking(true)
              break
              
            case 'response.done':
              updateStatus('connected')
              setIsAISpeaking(false)
              break
              
            case 'response.audio.delta':
              console.log('üé§ Chunk audio re√ßu de JARVIS')
              // Audio chunks from BETA API - traitement imm√©diat
              break
              
            case 'conversation.item.input_audio_transcription.completed':
              if (message.transcript) {
                updateTranscript(message.transcript)
              }
              break
              
            case 'response.output_text.delta':
              // Text chunks from GA API
              break
            
            // üéØ NOUVEAU : Function calling pour JARVIS VITRINE (commercial expert)
            case 'response.function_call_arguments.delta':
              // Arguments de fonction re√ßus progressivement
              console.log('üîß Function call arguments delta:', message)
              break
            
            case 'response.function_call_arguments.done':
              // Function call complet - EX√âCUTER
              console.log('üéØ Function call complet:', message)
              handleFunctionCall(message, dc)
              break
            
            case 'response.output_item.done':
              // Item termin√© (peut contenir un function call)
              if (message.item?.type === 'function_call') {
                console.log('‚úÖ Function call item done:', message.item.name)
              }
              break
              
            case 'error':
              console.error('‚ùå Erreur OpenAI:', message)
              setError(message.error?.message || 'Erreur de conversation')
              updateStatus('error')
              break
          }
        } catch (parseError) {
          console.error('‚ùå Erreur parsing message:', parseError)
        }
      }

      dc.onclose = () => {
        console.log('üì° Data channel ferm√©')
        setIsConnected(false)
        updateStatus('idle')
      }

      dc.onerror = (error) => {
        console.error('‚ùå Erreur data channel:', error)
        setError('Erreur de communication')
        updateStatus('error')
      }

      // Cr√©er et envoyer offre
      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Envoyer √† OpenAI (format BETA comme le kiosk)
      const ephemeralKey = session.client_secret.value
      console.log('üîë Token utilis√©:', ephemeralKey?.substring(0, 20) + '...')
      const realtimeResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17`, {
        method: 'POST',
        body: offer.sdp,
        headers: {
          'Authorization': `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
          'OpenAI-Beta': 'realtime=v1'
        },
      })

      if (!realtimeResponse.ok) {
        throw new Error(`WebRTC setup failed: ${realtimeResponse.status}`)
      }

      const answerSdp = await realtimeResponse.text()
      await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      console.log('‚úÖ WebRTC initialis√© avec succ√®s')
      
      // üí≥ Retourner les donn√©es de session
      return sessionResponse
      
    } catch (error: any) {
      console.error('‚ùå Erreur WebRTC:', error)
      
      let errorMessage = 'Erreur de connexion'
      
      switch (error.message) {
        case 'MICROPHONE_PERMISSION_DENIED':
          errorMessage = 'Veuillez autoriser l\'acc√®s au microphone'
          break
        case 'WebRTC non support√© par ce navigateur':
          errorMessage = 'Navigateur incompatible. Utilisez Chrome, Firefox ou Safari r√©cent'
          break
        default:
          if (error.name === 'NotAllowedError') {
            errorMessage = 'Acc√®s microphone refus√©'
          }
          break
      }
      
      setError(errorMessage)
      updateStatus('error')
      throw error
    }
  }, [createDemoSession, updateStatus, updateTranscript])

  // Connexion
  const connect = useCallback(async () => {
    if (isConnected) return
    
    setError(null)
    updateStatus('connecting')
    
    try {
      const sessionData = await initializeWebRTC()
      // üí≥ Retourner les donn√©es de session (incluant remainingCredits)
      return sessionData
    } catch (error) {
      console.error('Erreur de connexion:', error)
      // R√©initialiser l'√©tat en cas d'erreur pour √©viter la boucle
      updateStatus('error')
      setIsConnected(false)
      throw error
    }
  }, [isConnected, initializeWebRTC, updateStatus])

  // D√©connexion
  const disconnect = useCallback(async () => {
    try {
      // üîí NOUVEAU : Comptabiliser le temps de session
      if (sessionStartTimeRef.current) {
        const durationSeconds = Math.floor((Date.now() - sessionStartTimeRef.current) / 1000)
        console.log(`‚è±Ô∏è Dur√©e session: ${durationSeconds}s (${Math.ceil(durationSeconds / 60)} cr√©dits)`)
        
        // Appeler l'API pour enregistrer la dur√©e
        try {
          const response = await fetch('/api/voice/vitrine/end-session', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ durationSeconds })
          })
          
          if (response.ok) {
            console.log('‚úÖ Dur√©e session enregistr√©e')
          }
        } catch (error) {
          console.error('‚ùå Erreur enregistrement dur√©e:', error)
        }
      }
      
      // Fermer data channel
      if (dataChannelRef.current) {
        dataChannelRef.current.close()
        dataChannelRef.current = null
      }

      // Fermer peer connection
      if (peerConnectionRef.current) {
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }

      // Arr√™ter audio et nettoyer DOM
      if (audioElementRef.current) {
        audioElementRef.current.srcObject = null
        // Retirer du DOM
        if (audioElementRef.current.parentNode) {
          audioElementRef.current.parentNode.removeChild(audioElementRef.current)
        }
        audioElementRef.current = null
      }

      setIsConnected(false)
      setCurrentTranscript('')
      setIsAISpeaking(false)
      sessionStartTimeRef.current = null
      updateStatus('idle')
      
    } catch (error) {
      console.error('Erreur de d√©connexion:', error)
    }
  }, [updateStatus])

  // Nettoyage au d√©montage
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  // V√©rification timeout de session
  useEffect(() => {
    if (!isConnected || !sessionStartTimeRef.current) return

    const checkTimeout = () => {
      if (sessionStartTimeRef.current) {
        const elapsed = (Date.now() - sessionStartTimeRef.current) / 1000
        if (elapsed >= maxDurationRef.current) {
          disconnect()
        }
      }
    }

    const interval = setInterval(checkTimeout, 1000)
    return () => clearInterval(interval)
  }, [isConnected, disconnect])

  return {
    // √âtats
    isConnected,
    error,
    currentTranscript,
    isAISpeaking,
    
    // Actions
    connect,
    disconnect
  }
}
