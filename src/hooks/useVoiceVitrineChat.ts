"use client"

import { useState, useCallback, useRef, useEffect } from 'react'

interface VoiceVitrineConfig {
  onStatusChange?: (status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => void
  onTranscriptUpdate?: (transcript: string) => void
  maxDuration?: number // en secondes
}

export function useVoiceVitrineChat({
  onStatusChange,
  onTranscriptUpdate,
  maxDuration = 120
}: VoiceVitrineConfig) {
  // Ã‰tats
  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  
  // Refs pour WebRTC
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const sessionStartTimeRef = useRef<number | null>(null)
  const maxDurationRef = useRef(maxDuration)

  // Mettre Ã  jour maxDuration
  useEffect(() => {
    maxDurationRef.current = maxDuration
  }, [maxDuration])

  const updateStatus = useCallback((status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => {
    onStatusChange?.(status)
  }, [onStatusChange])

  const updateTranscript = useCallback((transcript: string) => {
    setCurrentTranscript(transcript)
    onTranscriptUpdate?.(transcript)
  }, [onTranscriptUpdate])

  // CrÃ©er une session Ã©phÃ©mÃ¨re pour la dÃ©mo
  const createDemoSession = useCallback(async () => {
    const sessionConfig = {
      session: {
        type: "realtime",
        model: "gpt-4o-mini-realtime-preview-2024-12-17",
        audio: {
          input: {
            format: {
              type: "audio/pcm",
              rate: 24000,
            },
            turn_detection: {
              type: "semantic_vad"
            }
          },
          output: {
            format: {
              type: "audio/pcm",
            },
            voice: "nova",
          }
        },
        instructions: `Tu es JARVIS, l'assistant IA de notre plateforme fitness rÃ©volutionnaire.

CONTEXTE DÃ‰MO VITRINE:
- C'est une dÃ©monstration de 2 minutes pour les visiteurs du site
- Tu reprÃ©sentes notre solution complÃ¨te pour les salles de sport
- Sois enthousiaste mais concis

PERSONNALITÃ‰:
- Accueillant et Ã©nergique
- Expert en fitness et technologie IA
- PassionnÃ© par l'innovation dans le sport

RÃ‰PONSES:
- Garde tes rÃ©ponses courtes (15-30 secondes max)
- Mets en avant les bÃ©nÃ©fices concrets de notre solution
- Invite Ã  dÃ©couvrir nos offres personnalisÃ©es
- Utilise des exemples concrets et inspirants

SUJETS Ã€ ABORDER SI PERTINENT:
- Analyse personnalisÃ©e des performances
- Recommandations d'entraÃ®nement intelligentes  
- Motivation adaptative basÃ©e sur l'IA
- Suivi de progression en temps rÃ©el
- Interface vocale dans les salles de sport

IMPORTANT: Cette dÃ©mo se termine automatiquement aprÃ¨s 2 minutes.`,
        output_modalities: ["audio", "text"]
      }
    }

    const response = await fetch('/api/voice/vitrine/session', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(sessionConfig),
    })

    if (!response.ok) {
      throw new Error(`Erreur session: ${response.status}`)
    }

    return await response.json()
  }, [])

  // Initialiser WebRTC
  const initializeWebRTC = useCallback(async () => {
    try {
      // VÃ©rifier support WebRTC
      if (!window.RTCPeerConnection) {
        throw new Error('WebRTC non supportÃ© par ce navigateur')
      }

      // CrÃ©er session dÃ©mo
      const session = await createDemoSession()
      
      // Configurer peer connection
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
      })
      peerConnectionRef.current = pc

      // Configurer audio output
      const audioElement = document.createElement('audio')
      audioElement.autoplay = true
      audioElement.playsInline = true
      audioElementRef.current = audioElement
      
      pc.ontrack = (event) => {
        if (audioElement) {
          audioElement.srcObject = event.streams[0]
        }
      }

      // Demander accÃ¨s microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        })
        
        // Ajouter track audio
        const audioTrack = stream.getAudioTracks()[0]
        pc.addTrack(audioTrack, stream)
      } catch (micError) {
        throw new Error('MICROPHONE_PERMISSION_DENIED')
      }

      // Configurer data channel
      const dc = pc.createDataChannel('oai-events')
      dataChannelRef.current = dc

      dc.onopen = () => {
        console.log('âœ… Data channel ouvert')
        setIsConnected(true)
        updateStatus('connected')
        sessionStartTimeRef.current = Date.now()
        
        // Configuration de session
        dc.send(JSON.stringify({
          type: 'session.update',
          session: {
            type: "realtime",
            instructions: session.session.instructions,
            output_modalities: ["audio", "text"]
          }
        }))
      }

      dc.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          console.log('ðŸ“¨ Message reÃ§u:', message.type)
          
          switch (message.type) {
            case 'input_audio_buffer.speech_started':
              updateStatus('listening')
              break
              
            case 'input_audio_buffer.speech_stopped':
              updateStatus('connected')
              break
              
            case 'response.created':
              updateStatus('speaking')
              setIsAISpeaking(true)
              break
              
            case 'response.done':
              updateStatus('connected')
              setIsAISpeaking(false)
              break
              
            case 'conversation.item.input_audio_transcription.completed':
              if (message.transcript) {
                updateTranscript(message.transcript)
              }
              break
              
            case 'error':
              console.error('âŒ Erreur OpenAI:', message)
              setError(message.error?.message || 'Erreur de conversation')
              updateStatus('error')
              break
          }
        } catch (parseError) {
          console.error('âŒ Erreur parsing message:', parseError)
        }
      }

      dc.onclose = () => {
        console.log('ðŸ“¡ Data channel fermÃ©')
        setIsConnected(false)
        updateStatus('idle')
      }

      dc.onerror = (error) => {
        console.error('âŒ Erreur data channel:', error)
        setError('Erreur de communication')
        updateStatus('error')
      }

      // CrÃ©er et envoyer offre
      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Envoyer Ã  OpenAI (format kiosk)
      const realtimeResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17`, {
        method: 'POST',
        body: offer.sdp,
        headers: {
          'Authorization': `Bearer ${session.client_secret.value}`,
          'Content-Type': 'application/sdp'
        },
      })

      if (!realtimeResponse.ok) {
        throw new Error(`WebRTC setup failed: ${realtimeResponse.status}`)
      }

      const answerSdp = await realtimeResponse.text()
      await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      console.log('âœ… WebRTC initialisÃ© avec succÃ¨s')
      
    } catch (error: any) {
      console.error('âŒ Erreur WebRTC:', error)
      
      let errorMessage = 'Erreur de connexion'
      
      switch (error.message) {
        case 'MICROPHONE_PERMISSION_DENIED':
          errorMessage = 'Veuillez autoriser l\'accÃ¨s au microphone'
          break
        case 'WebRTC non supportÃ© par ce navigateur':
          errorMessage = 'Navigateur incompatible. Utilisez Chrome, Firefox ou Safari rÃ©cent'
          break
        default:
          if (error.name === 'NotAllowedError') {
            errorMessage = 'AccÃ¨s microphone refusÃ©'
          }
          break
      }
      
      setError(errorMessage)
      updateStatus('error')
      throw error
    }
  }, [createDemoSession, updateStatus, updateTranscript])

  // Connexion
  const connect = useCallback(async () => {
    if (isConnected) return
    
    setError(null)
    updateStatus('connecting')
    
    try {
      await initializeWebRTC()
    } catch (error) {
      console.error('Erreur de connexion:', error)
      // RÃ©initialiser l'Ã©tat en cas d'erreur pour Ã©viter la boucle
      updateStatus('error')
      setIsConnected(false)
      throw error
    }
  }, [isConnected, initializeWebRTC, updateStatus])

  // DÃ©connexion
  const disconnect = useCallback(async () => {
    try {
      // Fermer data channel
      if (dataChannelRef.current) {
        dataChannelRef.current.close()
        dataChannelRef.current = null
      }

      // Fermer peer connection
      if (peerConnectionRef.current) {
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }

      // ArrÃªter audio
      if (audioElementRef.current) {
        audioElementRef.current.srcObject = null
        audioElementRef.current = null
      }

      setIsConnected(false)
      setCurrentTranscript('')
      setIsAISpeaking(false)
      sessionStartTimeRef.current = null
      updateStatus('idle')
      
    } catch (error) {
      console.error('Erreur de dÃ©connexion:', error)
    }
  }, [updateStatus])

  // Nettoyage au dÃ©montage
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  // VÃ©rification timeout de session
  useEffect(() => {
    if (!isConnected || !sessionStartTimeRef.current) return

    const checkTimeout = () => {
      if (sessionStartTimeRef.current) {
        const elapsed = (Date.now() - sessionStartTimeRef.current) / 1000
        if (elapsed >= maxDurationRef.current) {
          disconnect()
        }
      }
    }

    const interval = setInterval(checkTimeout, 1000)
    return () => clearInterval(interval)
  }, [isConnected, disconnect])

  return {
    // Ã‰tats
    isConnected,
    error,
    currentTranscript,
    isAISpeaking,
    
    // Actions
    connect,
    disconnect
  }
}
