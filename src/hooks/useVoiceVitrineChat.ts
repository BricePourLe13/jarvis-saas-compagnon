"use client"

import { useState, useCallback, useRef, useEffect } from 'react'
import { executeJarvisFunction } from '@/lib/jarvis-expert-functions'

interface VoiceVitrineConfig {
  onStatusChange?: (status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => void
  onTranscriptUpdate?: (transcript: string) => void
  maxDuration?: number // en secondes
}

export function useVoiceVitrineChat({
  onStatusChange,
  onTranscriptUpdate,
  maxDuration = 120
}: VoiceVitrineConfig) {
  // Ã‰tats
  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  
  // Refs pour WebRTC
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const sessionStartTimeRef = useRef<number | null>(null)
  const maxDurationRef = useRef(maxDuration)

  // Mettre Ã  jour maxDuration
  useEffect(() => {
    maxDurationRef.current = maxDuration
  }, [maxDuration])

  const updateStatus = useCallback((status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => {
    onStatusChange?.(status)
  }, [onStatusChange])

  const updateTranscript = useCallback((transcript: string) => {
    setCurrentTranscript(transcript)
    onTranscriptUpdate?.(transcript)
  }, [onTranscriptUpdate])

  // ðŸŽ¯ Handler pour les function calls (ROI, success stories, etc.)
  const handleFunctionCall = useCallback(async (message: any, dataChannel: RTCDataChannel) => {
    try {
      const { call_id, name, arguments: argsString } = message
      console.log(`ðŸ”§ ExÃ©cution function: ${name}`)
      console.log(`ðŸ“Š Arguments:`, argsString)
      
      // Parser les arguments
      const args = JSON.parse(argsString)
      
      // ExÃ©cuter la fonction experte
      const result = await executeJarvisFunction(name, args)
      console.log(`âœ… RÃ©sultat function ${name}:`, result)
      
      // Renvoyer le rÃ©sultat Ã  l'IA
      dataChannel.send(JSON.stringify({
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: call_id,
          output: JSON.stringify(result)
        }
      }))
      
      // Demander Ã  l'IA de rÃ©pondre avec ce rÃ©sultat
      dataChannel.send(JSON.stringify({
        type: 'response.create'
      }))
      
      console.log('ðŸ“¤ RÃ©sultat envoyÃ© Ã  JARVIS pour formulation')
      
    } catch (error) {
      console.error('âŒ Erreur exÃ©cution function call:', error)
      
      // En cas d'erreur, informer l'IA
      if (message.call_id) {
        dataChannel.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'function_call_output',
            call_id: message.call_id,
            output: JSON.stringify({ 
              error: 'Erreur lors du calcul, je vais vous donner une estimation gÃ©nÃ©rale.' 
            })
          }
        }))
        
        dataChannel.send(JSON.stringify({
          type: 'response.create'
        }))
      }
    }
  }, [])

  // CrÃ©er une session Ã©phÃ©mÃ¨re pour la dÃ©mo
  const createDemoSession = useCallback(async () => {
    // L'API attend maintenant directement la config sans wrapper "session"
    const response = await fetch('/api/voice/vitrine/session', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({}), // L'API crÃ©e la config elle-mÃªme maintenant
    })

    if (!response.ok) {
      // ðŸ”’ NOUVEAU : GÃ©rer les erreurs de limitation
      const errorData = await response.json().catch(() => ({}))
      
      const error: any = new Error(errorData.error || `Erreur session: ${response.status}`)
      error.statusCode = response.status
      error.hasActiveSession = errorData.hasActiveSession
      error.remainingCredits = errorData.remainingCredits
      error.isBlocked = errorData.isBlocked
      
      throw error
    }

    const sessionData = await response.json()
    console.log('âœ… Session crÃ©Ã©e:', sessionData)
    console.log('ðŸ” Structure session:', JSON.stringify(sessionData.session, null, 2))
    console.log('ðŸ” client_secret:', sessionData.session?.client_secret)
    
    // ðŸ’³ Retourner aussi les crÃ©dits restants
    if (sessionData.remainingCredits !== undefined) {
      console.log(`ðŸ’³ CrÃ©dits restants: ${sessionData.remainingCredits} minutes`)
    }
    
    return sessionData
  }, [])

  // Initialiser WebRTC
  const initializeWebRTC = useCallback(async () => {
    try {
      // VÃ©rifier support WebRTC
      if (!window.RTCPeerConnection) {
        throw new Error('WebRTC non supportÃ© par ce navigateur')
      }

      // CrÃ©er session dÃ©mo
      const sessionResponse = await createDemoSession()
      const session = sessionResponse.session
      console.log('ðŸ” Session utilisÃ©e:', session)
      
      // Configurer peer connection
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
      })
      peerConnectionRef.current = pc

      // CrÃ©er l'Ã©lÃ©ment audio pour le playback (COMME LE KIOSK)
      if (!audioElementRef.current) {
        const audioEl = document.createElement('audio')
        audioEl.autoplay = true
        audioElementRef.current = audioEl
      }

      // GÃ©rer l'audio entrant (rÃ©ponses de JARVIS) - COMME LE KIOSK
      pc.ontrack = (event) => {
        console.log('ðŸ”Š Audio entrant reÃ§u (style kiosk)')
        if (audioElementRef.current && event.streams[0]) {
          audioElementRef.current.srcObject = event.streams[0]
          console.log('âœ… Audio srcObject dÃ©fini')
        }
      }

      // Demander accÃ¨s microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 24000, // QualitÃ© HD pour meilleur rendu
            channelCount: 1,
            latency: 0.01, // Faible latence
            volume: 1.0
          }
        })
        
        // Ajouter track audio
        const audioTrack = stream.getAudioTracks()[0]
        pc.addTrack(audioTrack, stream)
      } catch (micError) {
        throw new Error('MICROPHONE_PERMISSION_DENIED')
      }

      // Configurer data channel
      const dc = pc.createDataChannel('oai-events')
      dataChannelRef.current = dc

      dc.onopen = () => {
        console.log('âœ… Data channel ouvert')
        setIsConnected(true)
        updateStatus('connected')
        sessionStartTimeRef.current = Date.now()
        
        // âœ… CORRECTION : Ne pas Ã©craser le prompt serveur !
        // Le prompt commercial expert est dÃ©jÃ  configurÃ© dans /api/voice/vitrine/session
        // avec toutes les instructions dÃ©taillÃ©es, function calling, etc.
        console.log('ðŸŽ¯ Utilisation du prompt commercial serveur (expert JARVIS-GROUP)')
      }

      dc.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          console.log('ðŸ“¨ Message reÃ§u:', message.type)
          
          switch (message.type) {
            case 'input_audio_buffer.speech_started':
              updateStatus('listening')
              break
              
            case 'input_audio_buffer.speech_stopped':
              updateStatus('connected')
              break
              
            case 'response.created':
              updateStatus('speaking')
              setIsAISpeaking(true)
              break
              
            case 'response.done':
              updateStatus('connected')
              setIsAISpeaking(false)
              break
              
            case 'response.audio.delta':
              console.log('ðŸŽ¤ Chunk audio reÃ§u de JARVIS')
              // Audio chunks from BETA API - traitement immÃ©diat
              break
              
            case 'conversation.item.input_audio_transcription.completed':
              if (message.transcript) {
                updateTranscript(message.transcript)
              }
              break
              
            case 'response.output_text.delta':
              // Text chunks from GA API
              break
            
            // ðŸŽ¯ NOUVEAU : Function calling pour JARVIS VITRINE (commercial expert)
            case 'response.function_call_arguments.delta':
              // Arguments de fonction reÃ§us progressivement
              console.log('ðŸ”§ Function call arguments delta:', message)
              break
            
            case 'response.function_call_arguments.done':
              // Function call complet - EXÃ‰CUTER
              console.log('ðŸŽ¯ Function call complet:', message)
              handleFunctionCall(message, dc)
              break
            
            case 'response.output_item.done':
              // Item terminÃ© (peut contenir un function call)
              if (message.item?.type === 'function_call') {
                console.log('âœ… Function call item done:', message.item.name)
              }
              break
              
            case 'error':
              console.error('âŒ Erreur OpenAI:', message)
              setError(message.error?.message || 'Erreur de conversation')
              updateStatus('error')
              break
          }
        } catch (parseError) {
          console.error('âŒ Erreur parsing message:', parseError)
        }
      }

      dc.onclose = () => {
        console.log('ðŸ“¡ Data channel fermÃ©')
        setIsConnected(false)
        updateStatus('idle')
      }

      dc.onerror = (error) => {
        console.error('âŒ Erreur data channel:', error)
        setError('Erreur de communication')
        updateStatus('error')
      }

      // CrÃ©er et envoyer offre
      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Envoyer Ã  OpenAI (format BETA comme le kiosk)
      const ephemeralKey = session.client_secret.value
      console.log('ðŸ”‘ Token utilisÃ©:', ephemeralKey?.substring(0, 20) + '...')
      const realtimeResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17`, {
        method: 'POST',
        body: offer.sdp,
        headers: {
          'Authorization': `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
          'OpenAI-Beta': 'realtime=v1'
        },
      })

      if (!realtimeResponse.ok) {
        throw new Error(`WebRTC setup failed: ${realtimeResponse.status}`)
      }

      const answerSdp = await realtimeResponse.text()
      await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      console.log('âœ… WebRTC initialisÃ© avec succÃ¨s')
      
      // ðŸ’³ Retourner les donnÃ©es de session
      return sessionResponse
      
    } catch (error: any) {
      console.error('âŒ Erreur WebRTC:', error)
      
      let errorMessage = 'Erreur de connexion'
      
      switch (error.message) {
        case 'MICROPHONE_PERMISSION_DENIED':
          errorMessage = 'Veuillez autoriser l\'accÃ¨s au microphone'
          break
        case 'WebRTC non supportÃ© par ce navigateur':
          errorMessage = 'Navigateur incompatible. Utilisez Chrome, Firefox ou Safari rÃ©cent'
          break
        default:
          if (error.name === 'NotAllowedError') {
            errorMessage = 'AccÃ¨s microphone refusÃ©'
          }
          break
      }
      
      setError(errorMessage)
      updateStatus('error')
      throw error
    }
  }, [createDemoSession, updateStatus, updateTranscript])

  // Connexion
  const connect = useCallback(async () => {
    if (isConnected) return
    
    setError(null)
    updateStatus('connecting')
    
    try {
      const sessionData = await initializeWebRTC()
      // ðŸ’³ Retourner les donnÃ©es de session (incluant remainingCredits)
      return sessionData
    } catch (error) {
      console.error('Erreur de connexion:', error)
      // RÃ©initialiser l'Ã©tat en cas d'erreur pour Ã©viter la boucle
      updateStatus('error')
      setIsConnected(false)
      throw error
    }
  }, [isConnected, initializeWebRTC, updateStatus])

  // DÃ©connexion
  const disconnect = useCallback(async () => {
    try {
      // ðŸ”’ NOUVEAU : Comptabiliser le temps de session
      if (sessionStartTimeRef.current) {
        const durationSeconds = Math.floor((Date.now() - sessionStartTimeRef.current) / 1000)
        console.log(`â±ï¸ DurÃ©e session: ${durationSeconds}s (${Math.ceil(durationSeconds / 60)} crÃ©dits)`)
        
        // Appeler l'API pour enregistrer la durÃ©e
        try {
          const response = await fetch('/api/voice/vitrine/end-session', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ durationSeconds })
          })
          
          if (response.ok) {
            console.log('âœ… DurÃ©e session enregistrÃ©e')
          }
        } catch (error) {
          console.error('âŒ Erreur enregistrement durÃ©e:', error)
        }
      }
      
      // Fermer data channel
      if (dataChannelRef.current) {
        dataChannelRef.current.close()
        dataChannelRef.current = null
      }

      // Fermer peer connection
      if (peerConnectionRef.current) {
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }

      // ArrÃªter audio et nettoyer DOM
      if (audioElementRef.current) {
        audioElementRef.current.srcObject = null
        // Retirer du DOM
        if (audioElementRef.current.parentNode) {
          audioElementRef.current.parentNode.removeChild(audioElementRef.current)
        }
        audioElementRef.current = null
      }

      setIsConnected(false)
      setCurrentTranscript('')
      setIsAISpeaking(false)
      sessionStartTimeRef.current = null
      updateStatus('idle')
      
    } catch (error) {
      console.error('Erreur de dÃ©connexion:', error)
    }
  }, [updateStatus])

  // Nettoyage au dÃ©montage
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  // VÃ©rification timeout de session
  useEffect(() => {
    if (!isConnected || !sessionStartTimeRef.current) return

    const checkTimeout = () => {
      if (sessionStartTimeRef.current) {
        const elapsed = (Date.now() - sessionStartTimeRef.current) / 1000
        if (elapsed >= maxDurationRef.current) {
          disconnect()
        }
      }
    }

    const interval = setInterval(checkTimeout, 1000)
    return () => clearInterval(interval)
  }, [isConnected, disconnect])

  return {
    // Ã‰tats
    isConnected,
    error,
    currentTranscript,
    isAISpeaking,
    
    // Actions
    connect,
    disconnect
  }
}
