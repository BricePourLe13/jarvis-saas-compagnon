/**
 * üéØ WHISPER PARALLEL TRACKER
 * 
 * STRAT√âGIE EXPERTE:
 * - User Speech ‚Üí Whisper API (transcription fiable + m√©tadonn√©es)
 * - AI Responses ‚Üí OpenAI Realtime transcript (d√©j√† disponible)
 * - Logging direct via API publique (pas de service role)
 * - Session lifecycle complet
 */

interface WhisperTrackingSession {
  session_id: string
  member_id: string
  gym_id: string
  turn_counter: number
  audio_chunks: Blob[]
  is_recording: boolean
}

class WhisperParallelTracker {
  private currentSession: WhisperTrackingSession | null = null
  private mediaRecorder: MediaRecorder | null = null
  private audioStream: MediaStream | null = null

  /**
   * üéØ Initialiser session de tracking Whisper
   */
  async initSession(sessionId: string, memberId: string, gymId: string) {
    this.currentSession = {
      session_id: sessionId,
      member_id: memberId,
      gym_id: gymId,
      turn_counter: 0,
      audio_chunks: [],
      is_recording: false
    }

    console.log('üéôÔ∏è [WHISPER TRACKER] Session initialis√©e:', sessionId)
    
    try {
      // Obtenir acc√®s microphone (si pas d√©j√† fait)
      this.audioStream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000 // Optimal pour Whisper
        } 
      })
      console.log('üé§ [WHISPER TRACKER] Microphone access obtained')
    } catch (error) {
      console.error('‚ùå [WHISPER TRACKER] Microphone access failed:', error)
    }
  }

  /**
   * üé§ D√©marrer enregistrement utilisateur
   */
  startUserRecording() {
    if (!this.currentSession || !this.audioStream || this.currentSession.is_recording) return

    try {
      this.mediaRecorder = new MediaRecorder(this.audioStream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      this.currentSession.audio_chunks = []
      this.currentSession.is_recording = true

      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0 && this.currentSession) {
          this.currentSession.audio_chunks.push(event.data)
        }
      }

      this.mediaRecorder.onstop = () => {
        this.processUserAudio()
      }

      this.mediaRecorder.start()
      console.log('üî¥ [WHISPER TRACKER] Recording started')
    } catch (error) {
      console.error('‚ùå [WHISPER TRACKER] Recording start failed:', error)
    }
  }

  /**
   * üõë Arr√™ter enregistrement utilisateur
   */
  stopUserRecording() {
    if (!this.currentSession || !this.mediaRecorder || !this.currentSession.is_recording) return

    this.currentSession.is_recording = false
    this.mediaRecorder.stop()
    console.log('‚èπÔ∏è [WHISPER TRACKER] Recording stopped')
  }

  /**
   * üéµ Traiter audio utilisateur avec Whisper
   */
  private async processUserAudio() {
    if (!this.currentSession || this.currentSession.audio_chunks.length === 0) return

    try {
      // Cr√©er blob audio
      const audioBlob = new Blob(this.currentSession.audio_chunks, { 
        type: 'audio/webm;codecs=opus' 
      })

      // V√©rifier taille minimale (√©viter silence)
      if (audioBlob.size < 1024) {
        console.log('‚ö†Ô∏è [WHISPER TRACKER] Audio too small, skipping')
        return
      }

      console.log('üì§ [WHISPER TRACKER] Sending audio to Whisper API...', {
        size: audioBlob.size,
        duration: 'unknown'
      })

      // Envoyer √† Whisper API
      const formData = new FormData()
      formData.append('audio', audioBlob, 'user-speech.webm')
      formData.append('session_id', this.currentSession.session_id)
      formData.append('member_id', this.currentSession.member_id)
      formData.append('gym_id', this.currentSession.gym_id)

      const response = await fetch('/api/whisper/transcribe-user', {
        method: 'POST',
        body: formData
      })

      if (response.ok) {
        const result = await response.json()
        console.log('‚úÖ [WHISPER TRACKER] User speech transcribed:', result.transcript?.substring(0, 50) + '...')
      } else {
        console.error('‚ùå [WHISPER TRACKER] Whisper API failed:', response.status)
      }

    } catch (error) {
      console.error('‚ùå [WHISPER TRACKER] Audio processing failed:', error)
    }
  }

  /**
   * ü§ñ Tracker r√©ponse IA (depuis OpenAI Realtime)
   */
  async trackAIResponse(transcript: string, metadata?: { latency_ms?: number, audio_quality?: string }) {
    if (!this.currentSession) return

    this.currentSession.turn_counter++

    try {
      const response = await fetch('/api/debug/log-interaction', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          session_id: this.currentSession.session_id,
          member_id: this.currentSession.member_id,
          gym_id: this.currentSession.gym_id,
          speaker: 'ai',
          transcript,
          conversation_turn_number: this.currentSession.turn_counter,
          metadata
        })
      })

      if (response.ok) {
        console.log(`ü§ñ [WHISPER TRACKER] AI response logged (turn ${this.currentSession.turn_counter}):`, transcript.substring(0, 50) + '...')
      }
    } catch (error) {
      console.error('‚ùå [WHISPER TRACKER] AI response logging failed:', error)
    }
  }

  /**
   * üèÅ Finaliser session
   */
  endSession(reason: string = 'user_goodbye') {
    if (!this.currentSession) return

    console.log(`üèÅ [WHISPER TRACKER] Session ended: ${this.currentSession.session_id} (Reason: ${reason})`)

    // Arr√™ter enregistrement si actif
    if (this.currentSession.is_recording) {
      this.stopUserRecording()
    }

    // Nettoyer ressources
    if (this.audioStream) {
      this.audioStream.getTracks().forEach(track => track.stop())
      this.audioStream = null
    }

    this.currentSession = null
    this.mediaRecorder = null
  }
}

export const whisperParallelTracker = new WhisperParallelTracker()
